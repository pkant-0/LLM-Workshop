{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 7279388,
          "sourceType": "datasetVersion",
          "datasetId": 4220596
        },
        {
          "sourceId": 4529,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 3321
        }
      ],
      "dockerImageVersionId": 30627,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "ðŸ”¥Gemini + StableDiffusion XL- Storys on-demandðŸ”¥ ",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkant-0/LLM-Workshop/blob/main/Gemini_%2B_StableDiffusion_XL_Storys_on_demand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'stable-diffusion-xl/pytorch/base-1-0/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F3321%2F4529%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T102404Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D18aeee5ea6535293c73db2d81d336c553d7945f5f1992db0ff18b2a30a9477226d4d95ebce65c3a159a744aec228b976d5c65023768962035d3ece97d9a4a26c8b8809cd604da9cab9bac07d0caf62329ba3df61353e555a566a9f0a91d1c68c0613d2a70496e5132bf31ac7781f60314ce0f3069a7ef1166d1d6dd60634601debf95ea0adaf5d30742554b68f5b3f9aac838bcada004750f15153300f70573e7e670f02ab9b8144803774a22542c41e69ae0e2623cae7b7cbaf7338fd843966468ddeac6f4786222e30fa4e89ef4d7337a96931745f1d0a3ba38b779379123e01f08e3d7bba6656e0e3c5ca5deaf5c4b6ac75795a17c9b0700e3eecc46a1287'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Qx8BFcB9nlSd"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color: #F0E68C; padding: 20px; border-radius: 10px; box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1);\">\n",
        "     <h1 style=\"font-family: 'Verdana'; color: #8B4513;\">First of all, click on RUN ALL in the menu above to start running the notebook since it takes more than 5 minutes to load, and make sure you have the GPU activated. When you finish reading this, the notebook, it will be ready to use!</h1>\n",
        "    <h1 style=\"font-family: 'Verdana'; color: #8B4513;\">First Month Project: Emulating \"Choose Your Own Adventure\" with Deep Learning - A Notebook for a Data Science Course\n",
        "\n",
        "Hello, everyone! This is a notebook I created for a data science course that I'm currently teaching. If anyone wants to join in, you're invited!\n",
        "\n",
        "Project Description ðŸš€\n",
        "\n",
        "The fourth project of the first month focuses on the use of Deep Learning and other Artificial Intelligence technologies to emulate the concept of \"Choose Your Own Adventure,\" inspired by interactive books where readers make choices that affect the direction of the story.\n",
        "\n",
        "Join Us! ðŸ“šðŸ¤–\n",
        "\n",
        "If you're interested in participating in this project, feel free to join us as we explore the exciting world of Data Science!\n",
        "\n",
        "## Note: This is an English Notebook\n",
        " ðŸ“šðŸ¤–</h1>\n",
        "    <p style=\"font-size: 18px; font-family: 'Verdana'; color: #8B4513; line-height: 1.5em;\">The fourth project of the first month focuses on using Deep Learning and other Artificial Intelligence technologies to emulate the concept of \"Choose Your Own Adventure,\" inspired by interactive books where readers make decisions that affect the direction of the story.</p>\n",
        "    <h2 style=\"font-family: 'Verdana'; color: #8B4513;\">Project Description ðŸš€</h2>\n",
        "    <p style=\"font-size: 18px; font-family: 'Verdana'; color: #8B4513; line-height: 1.5em;\">The goal of this project is to create an interactive experience where users can choose how the plot of a story unfolds. To achieve this, we will use various technologies, including Gemini, Stable Difusion, and Deep Learning-based Translators, as well as any other AI that students wish to employ.</p>\n",
        "    <h2 style=\"font-family: 'Verdana'; color: #8B4513;\">Process Flow (Pipeline) ðŸ”„</h2>\n",
        "    <ol style=\"font-size: 18px; font-family: 'Verdana'; color: #8B4513; line-height: 1.5em;\">\n",
        "        <li>The user initiates the experience with an initial prompt that sets the scene and the initial options.</li>\n",
        "        <li>The AI, through Gemini and other technologies, generates the story along with related images based on the initial prompt.</li>\n",
        "        <li>At at least two key points in the story, the program offers the user the opportunity to choose a plot twist.</li>\n",
        "        <li>The user's choice is incorporated into the script, affecting the future direction of the story.</li>\n",
        "        <li>The narrative continues according to the user's choices, generating interactive content based on the decisions made.</li>\n",
        "    </ol>\n",
        "    <h2 style=\"font-family: 'Verdana'; color: #8B4513;\">Resources and Delivery ðŸ“¦</h2>\n",
        "    <p style=\"font-size: 18px; font-family: 'Verdana'; color: #8B4513; line-height: 1.5em;\">In this notebook called \"Choose Your Own Adventure,\" which you are viewing, contains all the necessary functions for the project implementation. Additionally, a simple example of the program is included to guide you in development.</p>\n",
        "    <p style=\"font-size: 18px; font-family: 'Verdana'; color: #8B4513; line-height: 1.5em;\">This project is an exciting opportunity to apply the knowledge acquired in Deep Learning and explore creativity in the generation of interactive content. I hope you enjoy working on this unique Data Science experience!</p>\n",
        "    <p style=\"font-size: 18px; font-family: 'Verdana'; color: #8B4513; line-height: 1.5em;\">Good luck with your project! ðŸŒŸ</p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "9_53GMoenlSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"text-align:center\">\n",
        "    <h1 style=\"font-size:36px;\">Old book</h1>\n",
        "    <img src=\"https://th.bing.com/th/id/OIP.ludCT6xHOMk0TEWlnZYVwAHaKy?rs=1&pid=ImgDetMain\" />\n",
        "</div>"
      ],
      "metadata": {
        "id": "3pgi5uBvnlSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"text-align:center\">\n",
        "    <h1 style=\"font-size:36px;\">Your IA book!</h1>\n",
        "    <img src=\"https://raw.githubusercontent.com/kukedlc87/imagenes/main/Adventure'%20series..png\" />\n",
        "</div>"
      ],
      "metadata": {
        "id": "tZ_DPcninlSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Part of the Program\n",
        "## As advice, and what is always recommended when programming, is to think about a pipeline of the entire process and divide it into small parts to facilitate creation.\n",
        "- Think about the entire program.\n",
        "- Think about how you can break it down into smaller (functional) parts.\n",
        "- Then think about how the parts should be assembled.\n",
        "- Create a final pipeline that includes all the parts.\n",
        "- Get in a bad mood debugging the final pipeline :)\n",
        "- If you've achieved it and it works, NASA is probably looking for you!!!\n"
      ],
      "metadata": {
        "id": "88AuTJpDnlSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Tips for Writing Code:\n",
        "- Be neat (I'm not :))\n",
        "- Comment everything so that when you share it, the other person can understand it, or if you pick up the code a year later, you can understand what you did or attempted to do!\n",
        "- Be organized: all `pip install` commands in one place, all imports neatly arranged in another, and so on for each section of the code.\n",
        "- Use best practices (search for Python best practices on Google and the Zen of Python).\n",
        "- Save everything new you create and that works. We've all lost some code that we could never recreate, and it's really sad and frustrating. It happened to me with this very notebook (I never learn).\n"
      ],
      "metadata": {
        "id": "tjJbNguqnlSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `pip install` is a Python function for installing dependencies (libraries) that are not pre-installed on Kaggle.\n",
        "## Below, I am installing the Stable Diffusion dependency, Google Gemini, Torch (the framework we will use), and some utilities like the image viewer and other things that may be useful to you.\n"
      ],
      "metadata": {
        "id": "FxMSIuZRnlSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeaiv\n",
        "!pip install -q  transformers accelerate scipy safetensors diffusers\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:40:24.052202Z",
          "iopub.execute_input": "2023-12-26T03:40:24.052472Z",
          "iopub.status.idle": "2023-12-26T03:40:41.113668Z",
          "shell.execute_reply.started": "2023-12-26T03:40:24.052447Z",
          "shell.execute_reply": "2023-12-26T03:40:41.112523Z"
        },
        "trusted": true,
        "id": "KZTrGjhBnlSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports!"
      ],
      "metadata": {
        "id": "6XOhmL4UnlSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "from diffusers import DiffusionPipeline, EulerDiscreteScheduler\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:40:41.115679Z",
          "iopub.execute_input": "2023-12-26T03:40:41.115986Z",
          "iopub.status.idle": "2023-12-26T03:40:58.958316Z",
          "shell.execute_reply.started": "2023-12-26T03:40:41.115959Z",
          "shell.execute_reply": "2023-12-26T03:40:58.95755Z"
        },
        "trusted": true,
        "id": "qee3WJbMnlSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Clean Gemini's Response (to markdown) and API Key (you should have one by now) otherwise, you can use that.\n"
      ],
      "metadata": {
        "id": "xbCYlg6WnlSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "genai.configure(api_key='AIzaSyCp9t7h0VpF6Yh9whusANtTS0wpgzHbbXE')\n",
        "model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:40:58.959369Z",
          "iopub.execute_input": "2023-12-26T03:40:58.960074Z",
          "iopub.status.idle": "2023-12-26T03:40:58.965179Z",
          "shell.execute_reply.started": "2023-12-26T03:40:58.960045Z",
          "shell.execute_reply": "2023-12-26T03:40:58.964163Z"
        },
        "trusted": true,
        "id": "KLm3rvHRnlSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First, let's examine the functions separately. I'm going to try to recreate how I created the code.\n",
        "## My thoughts:\n",
        "- 1. I need something to generate stories: Gemini\n",
        "- 2. I need something to generate images for my stories: Stable Diffusion\n",
        "- 3. I need something to instruct Stable Diffusion on how the images should be: Gemini communicates with Stable Diffusion\n",
        "- 4. I need a function to repeat this process multiple times: Control structure - for loop\n",
        "- 5. I need a variable to store my story, possibly a dataframe to hold my stories, a summary, and the images: Pandas\n",
        "- 6. I need a control structure for the user to choose between two different outcomes\n",
        "- 7. I need to create a prompt notebook for each step of my pipeline.\n"
      ],
      "metadata": {
        "id": "q830fGj1nlSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. I need something to generate stories: Gemini\n",
        "## Note: Gemini may fail to respond; in class, we will learn when it happens, why it happens, and how to prevent it from happening!\n",
        "## If it gives an error, rerun the cell. All cells have been tested. If your cells fail and continue to fail when rerun, change your prompt!\n"
      ],
      "metadata": {
        "id": "juy1FkpKnlSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a story\n",
        "prompt = f\"I want you to create a story about two astronauts who arrive on a planet full of alien life and mystery with completely unknown forms...\"\n",
        "# Here I pass the prompt to the model\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(prompt)\n",
        "response.text\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:40:58.967997Z",
          "iopub.execute_input": "2023-12-26T03:40:58.968249Z",
          "iopub.status.idle": "2023-12-26T03:41:07.953281Z",
          "shell.execute_reply.started": "2023-12-26T03:40:58.968227Z",
          "shell.execute_reply": "2023-12-26T03:41:07.95231Z"
        },
        "trusted": true,
        "id": "FFuJ2M0BnlSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here I save the story in a variable to use it later to generate images with the context of the created story\n",
        "historia = response.text\n",
        "\n",
        "# Function to view the text better\n",
        "to_markdown(response.text)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:41:07.954391Z",
          "iopub.execute_input": "2023-12-26T03:41:07.954655Z",
          "iopub.status.idle": "2023-12-26T03:41:07.9616Z",
          "shell.execute_reply.started": "2023-12-26T03:41:07.954631Z",
          "shell.execute_reply": "2023-12-26T03:41:07.960692Z"
        },
        "trusted": true,
        "id": "eKCWXNWnnlSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. I need something to create images for my stories: Stable Diffusion\n",
        "## This process takes some time; if you add %%time at the beginning of the cell, it measures the execution time.\n"
      ],
      "metadata": {
        "id": "dxV1eBiWnlSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = DiffusionPipeline.from_pretrained(\"/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
        "pipe.to(\"cuda\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:41:07.962961Z",
          "iopub.execute_input": "2023-12-26T03:41:07.963459Z",
          "iopub.status.idle": "2023-12-26T03:42:27.228344Z",
          "shell.execute_reply.started": "2023-12-26T03:41:07.963423Z",
          "shell.execute_reply": "2023-12-26T03:42:27.227342Z"
        },
        "trusted": true,
        "id": "RK41wLTunlSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A function to create images without much hassle! You pass the prompt, and it gives you the image! Below are usage examples.\n",
        "def generate(prompt):\n",
        "    return pipe(prompt=prompt).images[0]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:42:27.22968Z",
          "iopub.execute_input": "2023-12-26T03:42:27.230017Z",
          "iopub.status.idle": "2023-12-26T03:42:27.23434Z",
          "shell.execute_reply.started": "2023-12-26T03:42:27.229983Z",
          "shell.execute_reply": "2023-12-26T03:42:27.233412Z"
        },
        "trusted": true,
        "id": "ZjDY5-vMnlSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Many times, an English prompt works better than a Spanish one. News: Gemini also translates later; there's an example below.\n",
        "# Another thing: for generating images, prompt engineering also exists. Note in the prompt below the following:\n",
        "# BREAKDOWN\n",
        "# context: A Villain Character\n",
        "# visual style: 8K hyper-detailed world - luminescent atmospheres - cinematic fantasy realm\n",
        "# details: showcases stunning chiaroscuro visuals\n",
        "# instruction: and ideal composition crafting a mysterious\n",
        "\n",
        "prompt = \"A Villain Character in an 8K hyper-detailed world, showcases stunning chiaroscuro visuals, luminescent atmospheres, and ideal composition, crafting a mysterious, cinematic fantasy realm\"\n",
        "plt.imshow(generate(prompt))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:42:27.235512Z",
          "iopub.execute_input": "2023-12-26T03:42:27.235784Z",
          "iopub.status.idle": "2023-12-26T03:43:12.266Z",
          "shell.execute_reply.started": "2023-12-26T03:42:27.235761Z",
          "shell.execute_reply": "2023-12-26T03:43:12.265061Z"
        },
        "trusted": true,
        "id": "x-clKxATnlSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matplotlib function to enlarge the image since they are in high definition!\n",
        "## Note: I just tried Stable XL; it's insane. I was using a very old version that left much to be desired!\n"
      ],
      "metadata": {
        "id": "6kfHqtjEnlSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"A Villain Character in an 8K hyper-detailed world, showcases stunning chiaroscuro visuals, luminescent atmospheres, and ideal composition, crafting a mysterious, cinematic fantasy realm\"\n",
        "image = generate(prompt)\n",
        "# Define the figure size\n",
        "plt.figure(figsize=(14, 12))  # You can adjust the size according to your preferences\n",
        "\n",
        "# Show the image in the figure\n",
        "plt.imshow(image)\n",
        "\n",
        "# Adjust the axes if necessary\n",
        "plt.axis('off')  # This will hide the axes if they are not needed\n",
        "\n",
        "# Display the image in the current window\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:43:12.267158Z",
          "iopub.execute_input": "2023-12-26T03:43:12.267446Z",
          "iopub.status.idle": "2023-12-26T03:43:53.184978Z",
          "shell.execute_reply.started": "2023-12-26T03:43:12.267421Z",
          "shell.execute_reply": "2023-12-26T03:43:53.183848Z"
        },
        "trusted": true,
        "id": "RUA8GHkTnlSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. I need something to instruct Stable Diffusion on how the images should be: Gemini communicates with Stable Diffusion\n"
      ],
      "metadata": {
        "id": "j9MfwlP2nlSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create an image in Stable XL from the original story (previously saved in the variable 'historia') translated into English, applying everything learned so far, including prompt engineering.\n",
        "prompt = f\"I want you to create an English prompt with this story {historia} with the following texts: 8K hyper-detailed world - luminescent atmospheres - cinematic fantasy realm, with a maximum of 50 words.\"\n",
        "# Here I pass the prompt to the model\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(prompt)\n",
        "# Save the prompt to pass it to Stable Diffusion\n",
        "prompt_for_image = response.text\n",
        "to_markdown(response.text)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:43:53.188242Z",
          "iopub.execute_input": "2023-12-26T03:43:53.188551Z",
          "iopub.status.idle": "2023-12-26T03:43:58.417387Z",
          "shell.execute_reply.started": "2023-12-26T03:43:53.188524Z",
          "shell.execute_reply": "2023-12-26T03:43:58.416435Z"
        },
        "trusted": true,
        "id": "BWx9ZRAdnlSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to create an image in Stable XL from the original story (previously saved in the 'historia' variable) translated into English, applying everything learned so far, including prompt engineering.\n",
        "prompt = f\"I want you to create an English prompt with this story {historia} with the following texts: 8K hyper-detailed world - luminescent atmospheres - cinematic fantasy realm, with a maximum of 50 words.\"\n",
        "# Here I pass the prompt to the model\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(prompt)\n",
        "# Save the prompt to pass it to Stable Diffusion\n",
        "prompt_for_image = response.text\n",
        "to_markdown(response.text)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:43:58.418628Z",
          "iopub.execute_input": "2023-12-26T03:43:58.419019Z",
          "iopub.status.idle": "2023-12-26T03:44:00.997362Z",
          "shell.execute_reply.started": "2023-12-26T03:43:58.418982Z",
          "shell.execute_reply": "2023-12-26T03:44:00.996581Z"
        },
        "trusted": true,
        "id": "67i23ZNTnlSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = generate(prompt_for_image)\n",
        "plt.figure(figsize=(14, 12))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:44:00.99843Z",
          "iopub.execute_input": "2023-12-26T03:44:00.998705Z",
          "iopub.status.idle": "2023-12-26T03:44:43.826704Z",
          "shell.execute_reply.started": "2023-12-26T03:44:00.998681Z",
          "shell.execute_reply": "2023-12-26T03:44:43.825818Z"
        },
        "trusted": true,
        "id": "gIvcrBO-nlSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# George Lucas would like this!"
      ],
      "metadata": {
        "id": "zrAG7EKNnlSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. I need a function for this process to repeat several times: Control structure - for loop\n"
      ],
      "metadata": {
        "id": "RDG7yRpynlSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a story\n",
        "prompt = f\"I want you to create a story about two astronauts who arrive on an unknown and mysterious planet with completely unknown life...\"\n",
        "# Here I pass the prompt to the model\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(prompt)\n",
        "response.text\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:57:59.576116Z",
          "iopub.execute_input": "2023-12-26T03:57:59.576377Z",
          "iopub.status.idle": "2023-12-26T03:58:10.036329Z",
          "shell.execute_reply.started": "2023-12-26T03:57:59.576355Z",
          "shell.execute_reply": "2023-12-26T03:58:10.035333Z"
        },
        "trusted": true,
        "id": "FnCWz-8RnlSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "historia = response.text\n",
        "to_markdown(historia)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:58:54.201767Z",
          "iopub.execute_input": "2023-12-26T03:58:54.202662Z",
          "iopub.status.idle": "2023-12-26T03:58:54.208452Z",
          "shell.execute_reply.started": "2023-12-26T03:58:54.202626Z",
          "shell.execute_reply": "2023-12-26T03:58:54.207551Z"
        },
        "trusted": true,
        "id": "9bQc4dRFnlSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to continue the story\n",
        "prompt = f\"I want you to continue this story: {historia} leaving the ending open and following the plot.\"\n",
        "# Here I pass the prompt to the model\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(prompt)\n",
        "to_markdown(response.text)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:59:00.207019Z",
          "iopub.execute_input": "2023-12-26T03:59:00.207447Z",
          "iopub.status.idle": "2023-12-26T03:59:08.555386Z",
          "shell.execute_reply.started": "2023-12-26T03:59:00.207411Z",
          "shell.execute_reply": "2023-12-26T03:59:08.554336Z"
        },
        "trusted": true,
        "id": "x9r3n8jtnlSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create an image in Stable XL from the original story (previously saved in the 'historia' variable), translated into English, applying everything learned so far, including prompt engineering.\n",
        "prompt = f\"I want you to create an English prompt with this story {response.text} with the following aesthetic criteria: 8K hyper-detailed world - luminescent atmospheres - cinematic fantasy realm, with a maximum of 50 words.\"\n",
        "# Here I pass the prompt to the model\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(prompt)\n",
        "# Save the prompt to pass it to Stable Diffusion\n",
        "prompt_for_image = response.text\n",
        "imagen = generate(prompt_for_image)\n",
        "plt.figure(figsize=(14, 12))  # You can adjust the size according to your preferences\n",
        "plt.imshow(imagen)\n",
        "plt.axis('off')  # This will hide the axes if they are not needed\n",
        "plt.show()\n",
        "to_markdown(response.text)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T04:00:58.504054Z",
          "iopub.execute_input": "2023-12-26T04:00:58.504924Z",
          "iopub.status.idle": "2023-12-26T04:01:44.460205Z",
          "shell.execute_reply.started": "2023-12-26T04:00:58.504892Z",
          "shell.execute_reply": "2023-12-26T04:01:44.459195Z"
        },
        "trusted": true,
        "id": "g_2-c0BFnlSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Xtras funktions!!!"
      ],
      "metadata": {
        "id": "XQmW8BHynlSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Function to create images and display 2 in 1!\n",
        "\n",
        "def imagine(prompt):  # Midjourney style!!!\n",
        "    image = generate(prompt)\n",
        "    plt.figure(figsize=(14, 12))\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:45:45.078422Z",
          "iopub.execute_input": "2023-12-26T03:45:45.078709Z",
          "iopub.status.idle": "2023-12-26T03:45:45.083855Z",
          "shell.execute_reply.started": "2023-12-26T03:45:45.078683Z",
          "shell.execute_reply": "2023-12-26T03:45:45.082848Z"
        },
        "trusted": true,
        "id": "K3vLI0u_nlSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simpaty error! Null Prompt = IA Original ART"
      ],
      "metadata": {
        "id": "2XZyniR8nlSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imagine('')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:45:45.084993Z",
          "iopub.execute_input": "2023-12-26T03:45:45.08532Z",
          "iopub.status.idle": "2023-12-26T03:46:28.548822Z",
          "shell.execute_reply.started": "2023-12-26T03:45:45.085282Z",
          "shell.execute_reply": "2023-12-26T03:46:28.547892Z"
        },
        "trusted": true,
        "id": "R743g7zDnlSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using prompt engineering on Images you can achieve very beautiful results!"
      ],
      "metadata": {
        "id": "m6SnKZ5rnlSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imagine('An Villian Character with body on fire in an 8K hyper-detailed world, showcases stunning chiaroscuro visuals, luminescent atmospheres, and ideal composition, crafting a mysterious, cinematic fantasy realm')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:46:28.550033Z",
          "iopub.execute_input": "2023-12-26T03:46:28.550319Z",
          "iopub.status.idle": "2023-12-26T03:47:12.07283Z",
          "shell.execute_reply.started": "2023-12-26T03:46:28.550293Z",
          "shell.execute_reply": "2023-12-26T03:47:12.071967Z"
        },
        "trusted": true,
        "id": "TJil3FQunlSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imagine('In the heart of Paris, the Eiffel Tower stands tall against the city lights. Though the Northern Lights rarely visit this bustling metropolis, the towerâ€™s graceful silhouette captivates with its nightly luminosity, drawing admirers to marvel at its beauty amidst the urban bustle.')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:47:12.074054Z",
          "iopub.execute_input": "2023-12-26T03:47:12.074341Z",
          "iopub.status.idle": "2023-12-26T03:47:55.593155Z",
          "shell.execute_reply.started": "2023-12-26T03:47:12.074316Z",
          "shell.execute_reply": "2023-12-26T03:47:55.591973Z"
        },
        "trusted": true,
        "id": "qchqsmVunlSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imagine('Elegant and beauty Bollywood actress,draped in a luxurious gold-colored one-piece outfit with a sophisticated belt cinching the waist,adorned with an ornate headpiece featuring a prominent feather, capturing the essence of grace and poise,overall style should reflect a blend of traditional and contemporary fashion, suitable for a grand event or celebration ,Perfect Face, Perfect eyes, Perfect Anatomy You can Change few elements')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:47:55.594722Z",
          "iopub.execute_input": "2023-12-26T03:47:55.595072Z",
          "iopub.status.idle": "2023-12-26T03:48:39.107384Z",
          "shell.execute_reply.started": "2023-12-26T03:47:55.595043Z",
          "shell.execute_reply": "2023-12-26T03:48:39.106431Z"
        },
        "trusted": true,
        "id": "M7G2Pe4tnlS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create multiples images\n",
        "for i in range (5):\n",
        "    imagine('Design a bustling cityscape merging futuristic skyscrapers with lush green parks, showcasing a harmonious coexistence of nature and technology in a metropolis of the future')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:48:39.108562Z",
          "iopub.execute_input": "2023-12-26T03:48:39.10886Z",
          "iopub.status.idle": "2023-12-26T03:52:16.543582Z",
          "shell.execute_reply.started": "2023-12-26T03:48:39.108834Z",
          "shell.execute_reply": "2023-12-26T03:52:16.542609Z"
        },
        "trusted": true,
        "id": "GGa73NzunlS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s8i005HInlS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic = input('Choose any and push enter!')\n",
        "prompt = f\"I want you to create an English prompt with context, aesthetic criteria, lighting and shadows, and artistic style for this topic {topic} that does not exceed 50 words and is in the English language.\"\n",
        "# Here I pass the prompt to the model\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(prompt)\n",
        "imagine(response.text)\n",
        "to_markdown(response.text)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:52:16.544857Z",
          "iopub.execute_input": "2023-12-26T03:52:16.545185Z",
          "iopub.status.idle": "2023-12-26T03:55:37.406805Z",
          "shell.execute_reply.started": "2023-12-26T03:52:16.545157Z",
          "shell.execute_reply": "2023-12-26T03:55:37.405915Z"
        },
        "trusted": true,
        "id": "pLnO_olrnlS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other function"
      ],
      "metadata": {
        "id": "h-f7_tOFnlS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic = input('Choose a theme and press enter!')\n",
        "number = int(input('Choose how many images you want to generate and press enter!'))\n",
        "\n",
        "for i in range(number):\n",
        "    prompt = f\"I want you to create an English prompt with context, aesthetic criteria, lighting and shadows, and artistic style for this theme {topic} that does not exceed 50 words and is in the English language.\"\n",
        "    # Here I pass the prompt to the model\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "    response = model.generate_content(prompt)\n",
        "    imagine(response.text)\n",
        "    to_markdown(response.text)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T03:55:37.40826Z",
          "iopub.execute_input": "2023-12-26T03:55:37.408879Z",
          "iopub.status.idle": "2023-12-26T03:57:48.776939Z",
          "shell.execute_reply.started": "2023-12-26T03:55:37.408845Z",
          "shell.execute_reply": "2023-12-26T03:57:48.775972Z"
        },
        "trusted": true,
        "id": "LW9sJP16nlS3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}